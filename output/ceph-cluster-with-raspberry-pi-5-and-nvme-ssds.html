<!DOCTYPE html>

<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
<link href="http://gmpg.org/xfn/11" rel="profile"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<!-- Metadata -->
<meta content="Bare metal provisioning of a Ceph cluster with three Raspberry Pi 5 and NVMe SSDs" name="description"/>
<meta content="Bare metal provisioning of a Ceph cluster with three Raspberry Pi 5 and NVMe SSDs" property="og:description"/>
<meta content="Ceph Cluster with Raspberry Pi 5 and NVMe SSDs" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="http://127.0.0.1:8000/ceph-cluster-with-raspberry-pi-5-and-nvme-ssds.html" property="og:url"/>
<meta content="https://max-pfeiffer.github.io/blog/images/2024-12-26_ceph_cluster.jpeg" property="og:image"/>
<!-- Enable responsiveness on mobile devices-->
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1" name="viewport">
<title>The Nerdy Tech Blog</title>
<!-- CSS -->
<link href="//fonts.googleapis.com/" rel="dns-prefetch"/>
<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/poole.css" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/hyde.css" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/syntax.css" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" rel="stylesheet">
<!-- Feeds -->
<!-- Analytics -->
</link></meta><meta content="F3nYemi_dYYQckoOJ33wBp5BG4frj27_I6Iiianw594" name="google-site-verification"/><link href="/favicon.ico" rel="icon"/><link href="http://127.0.0.1:8000/ceph-cluster-with-raspberry-pi-5-and-nvme-ssds.html" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "The Nerdy Tech Blog", "item": "http://127.0.0.1:8000"}, {"@type": "ListItem", "position": 2, "name": "Ceph cluster with raspberry pi 5 and nvme ssds", "item": "http://127.0.0.1:8000/ceph-cluster-with-raspberry-pi-5-and-nvme-ssds.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "Max Pfeiffer"}, "publisher": {"@type": "Organization", "name": "The Nerdy Tech Blog", "logo": {"@type": "ImageObject", "url": "https://max-pfeiffer.github.io/blog/images/avatar.jpeg"}}, "headline": "Ceph Cluster with Raspberry Pi 5 and NVMe SSDs", "about": "misc", "datePublished": "2024-12-26 10:00", "image": "https://max-pfeiffer.github.io/blog/images/2024-12-26_ceph_cluster.jpeg"}</script></head>
<body class="theme-base-0d">
<div class="sidebar">
<div class="container sidebar-sticky">
<div class="sidebar-about">
<h1>
<a href="http://127.0.0.1:8000/">
<img class="profile-picture" src="http://127.0.0.1:8000/images/avatar.jpeg"/>
					The Nerdy Tech Blog
				</a>
</h1>
<p class="lead"></p>
<p class="lead"> </p>
<p></p>
</div>
<ul class="sidebar-nav">
<li><a href="http://127.0.0.1:8000/pages/about.html">About</a></li>
<li><a href="http://127.0.0.1:8000/pages/imprint.html">Imprint</a></li>
<li><a href="http://127.0.0.1:8000/pages/privacy.html">Privacy</a></li>
</ul>
<nav class="sidebar-social">
<a class="sidebar-social-item" href="https://github.com/max-pfeiffer" target="_blank">
<i class="fa fa-github"></i>
</a>
<a class="sidebar-social-item" href="http://127.0.0.1:8000/">
<i class="fa fa-rss"></i>
</a>
</nav>
<p class="sidebar-footer">Â© 2025 Max Pfeiffer</p>
</div>
</div> <div class="content container">
<div class="post">
<h1 class="post-title">Ceph Cluster with Raspberry Pi 5 and NVMe SSDs</h1>
<span class="post-date">Thu 26 December 2024</span>
<p>As described in an earlier article, I run a Kubernetes cluster on my
<a href="https://www.proxmox.com/en/proxmox-virtual-environment/overview">Proxmox VE hypervisor</a> using Talos Linux. For this
Kubernetes cluster I built a <a href="https://ceph.io/">Ceph</a> cluster as a storage solution. This time I did not want to do this
with virtual machines using my Hypervisor. I choose to set it up on bare metal in order to learn a bit how
<a href="https://ceph.io/">Ceph</a> deals with hardware and different storage devices. Doing this, I ran into some bugs during Ceph
installation and provisioning the <a href="https://github.com/ceph/ceph-csi">Ceph CSI</a> on the Kubernetes cluster. So I decided
to share my experience with it. I might help someone out there and save him some time.</p>
<p><a href="https://www.proxmox.com/en/proxmox-virtual-environment/overview">Proxmox VE hypervisor</a> offers its own
<a href="https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster">Ceph cluster solution</a>. But this is geared towards
providing storage pools for virtual machines on that hypervisor. It also just deploys one
<a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Monitor">Ceph monitor</a> per hypervisor host. The
<a href="https://docs.ceph.com/en/reef/glossary/#term-Ceph-Monitor">Ceph monitor</a> maintains a map of the state of the cluster.
You need at least three monitors in order to be redundant and highly available. So if you have only one Promox VE
server running (like me), this is not a good option. Plus that Ceph installation is tightly coupled and
intermingled with Proxmox VE. So tinkering with that configuration and cluster authentication was causing problems
when I tried it out.</p>
<h2>Hardware</h2>
<p>I choose to use Raspberry Pi 5 as a platform because they have enough computing power and RAM for a small lab cluster
with little usage/traffic. Since a while these add on HATs for NVMe SSDs are available for it, so I thought that
could be a speedy and affordable way to handle storage. I basically threw all of this then in a 10'' desktop rack
together with a POE switch.</p>
<p><img alt="Ceph Cluster in 10'' rack" src="http://127.0.0.1:8000/images/2024-12-26_ceph_cluster.jpeg"/></p>
<p>Parts list:</p>
<ul>
<li><a href="https://deskpi.com/products/deskpi-rackmate-t1-2">DeskPi RackMate T1 Rackmount 10''</a></li>
<li><a href="https://store.ui.com/us/en/products/es-8-150w">Ubiquiti Es-8-150w POE Switch</a></li>
<li>3x <a href="https://www.raspberrypi.com/products/raspberry-pi-5/">Raspberry Pi 5</a></li>
<li>3x <a href="https://www.waveshare.com/poe-hat-f.htm">Waveshare POE HAT (F)</a></li>
<li>3x <a href="https://www.waveshare.com/pcie-to-m.2-board-d.htm">Waveshare PCIe To M.2 Adapter Board (D)</a></li>
<li>3x <a href="https://www.digitec.ch/en/s1/product/sandisk-extreme-microsdxc-microsdxc-64-gb-u3-uhs-i-memory-card-20932252">SanDisk Extreme microSDXC, 64 GB, U3, UHS-I</a></li>
<li>3x <a href="https://www.kingston.com/en/ssd/nv3-nvme-pcie-ssd">Kingston NV3 NVMe SSD 1000GB</a></li>
</ul>
<h2>Installing Ubuntu on Raspberry Pis</h2>
<p>Checking on the <a href="https://docs.ceph.com/en/reef/start/os-recommendations/">OS recommendations for Ceph</a> I learned that
Ubuntu should be a good option: Ubuntu 22.04 was tested with Ceph v18.2.
The problem is that for Raspberry Pi 5 only Ubuntu 24.04 is available. Ubuntu 22.04 will not be backported for
Raspberry Pi 5. So I flashed Ubuntu 24.04 onto the micro SD cards using the <a href="https://www.raspberrypi.com/software/">Raspberry Pi imager</a>.
I found it quite usefully to use the customization options of Raspberry Pi imager and provision hostnames and SSH keys
when flashing the images.</p>
<h3>The Cephadm Bug in v19.2.0</h3>
<p><a href="https://docs.ceph.com/en/reef/install/#recommended-methods">The recommended installation method is nowadays cephadm.</a>
I noticed that only Docker was missing from <a href="https://docs.ceph.com/en/reef/cephadm/install/#requirements">requirements</a>
after installing Ubuntu. So I quickly installed it on all nodes:</p>
<div class="highlight"><pre><span></span><code>apt<span class="w"> </span>update
apt<span class="w"> </span>install<span class="w"> </span>docker.io
</code></pre></div>
<p>You need to pick one of the Raspberry Pis to be your admin node. Then you just
<a href="https://docs.ceph.com/en/reef/cephadm/install/#install-cephadm">install cephadm</a> on that node.  I choose to install
the ubuntu package for simplicity:</p>
<div class="highlight"><pre><span></span><code>apt<span class="w"> </span>install<span class="w"> </span>cephadm
</code></pre></div>
<p>That installs the v19.2.0 version on Ubuntu 24.04 currently. But that version is not tested with this Ubuntu version
(see above), and this is a problem. ðŸ˜€ So there is a <a href="https://tracker.ceph.com/issues/66389">bug in the v19.2.0 cephadm version</a>,
where it is not able to parse AppArmor profiles in <code>/sys/kernel/security/apparmor/profiles</code>. This will cause you a
multitude of problems: storage devices will not become discovered, cluster communication is flawed etc.. Please check
on <a href="https://stackoverflow.com/questions/78743144/ceph-faild-to-add-osd-node-to-a-new-ceph-cluster-error-einval-traceback-most">the Stackoverflow thread</a>
for more details. The <a href="https://tracker.ceph.com/issues/66530">fix</a> is already on its way for cephadm v19.2.1.</p>
<p>Besides cephadm I choose not to install additional ceph packages (i.e., ceph, ceph-volume), as you can run almost any
admin command with <code>cephadm shell</code>.</p>
<h3>The Workaround</h3>
<p>As this problem is caused by the MongoDB Compass profile containing spaces in the name, I choose to disable that
Apparmor profile as workaround. I am not using MongoDB on the machines, so this should not be a problem. You need to
apply the workaround on all Raspberry Pis:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>ln<span class="w"> </span>-s<span class="w"> </span>/etc/apparmor.d/MongoDB_Compass<span class="w"> </span>/etc/apparmor.d/disable/
sudo<span class="w"> </span>apparmor_parser<span class="w"> </span>-R<span class="w"> </span>/etc/apparmor.d/MongoDB_Compass
sudo<span class="w"> </span>systemctl<span class="w"> </span>reload<span class="w"> </span>apparmor.service
</code></pre></div>
<p>Check that the MongoDB_Compass profile became disabled:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>systemctl<span class="w"> </span>status<span class="w"> </span>apparmor.service
sudo<span class="w"> </span>cat<span class="w"> </span>/sys/kernel/security/apparmor/profiles<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>MongoDB
</code></pre></div>
<h2>Boostrap Ceph cluster</h2>
<p><a href="https://docs.ceph.com/en/reef/cephadm/install/#bootstrap-a-new-cluster">Bootstrapping the Ceph Cluster with cephadm</a>
on your admin node is straight forward, the IP address is the one of your admin node:</p>
<div class="highlight"><pre><span></span><code>cephadm<span class="w"> </span>bootstrap<span class="w"> </span>--mon-ip<span class="w"> </span><span class="m">192</span>.168.1.100
</code></pre></div>
<h2>Adding Hosts</h2>
<p>My node layout looks like this:</p>
<ul>
<li>hostname: ceph1, IP: 192.168.1.100, admin node </li>
<li>hostname: ceph2, IP: 192.168.1.101</li>
<li>hostname: ceph3, IP: 192.168.1.102</li>
</ul>
<p>Cephadm needs a user which needs a certain set of permissions. By default, the root user is used, but
<a href="https://docs.ceph.com/en/octopus/cephadm/operations/#configuring-a-different-ssh-user">you can also configure a different user with narrowed down permissions</a>.
I am just going with the default here.</p>
<p>In order to add the other two nodes, you need to configure the sshd in <code>/etc/ssh/sshd_config</code> and permit root login:
<code>PermitRootLogin yes</code> or better <code>PermitRootLogin prohibit-password</code>. After that, you can copy over the SSH keys from the
admin node like that:</p>
<div class="highlight"><pre><span></span><code>ssh-copy-id<span class="w"> </span>-f<span class="w"> </span>-i<span class="w"> </span>/etc/ceph/ceph.pub<span class="w"> </span>root@ceph2
ssh-copy-id<span class="w"> </span>-f<span class="w"> </span>-i<span class="w"> </span>/etc/ceph/ceph.pub<span class="w"> </span>root@ceph3
</code></pre></div>
<p><a href="https://docs.ceph.com/en/reef/cephadm/host-management/#adding-hosts">Add the hosts like this</a>:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>cephadm<span class="w"> </span>shell<span class="w"> </span>ceph<span class="w"> </span>orch<span class="w"> </span>host<span class="w"> </span>add<span class="w"> </span>ceph2<span class="w"> </span><span class="m">192</span>.168.1.101
sudo<span class="w"> </span>cephadm<span class="w"> </span>shell<span class="w"> </span>ceph<span class="w"> </span>orch<span class="w"> </span>host<span class="w"> </span>add<span class="w"> </span>ceph3<span class="w"> </span><span class="m">192</span>.168.1.102
</code></pre></div>
<p>On these two nodes Ceph will add a monitor each, so you then have in total three monitors and a highly available Ceph
cluster.</p>
<h2>Adding Devices and Object Storage Daemons (OSD)</h2>
<p>Please be aware that you can only add new devices if these
<a href="https://docs.ceph.com/en/reef/cephadm/services/osd/#listing-storage-devices">requirements</a> are met:</p>
<ul>
<li>The device must have no partitions.</li>
<li>The device must not have any LVM state.</li>
<li>The device must not be mounted.</li>
<li>The device must not contain a file system. </li>
<li>The device must not contain a Ceph BlueStore OSD. </li>
<li>The device must be larger than 5 GB.</li>
</ul>
<p>So you better check if this is actually the case:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>cephadm<span class="w"> </span>shell<span class="w"> </span>ceph<span class="w"> </span>orch<span class="w"> </span>device<span class="w"> </span>ls<span class="w"> </span>--wide<span class="w"> </span>--refresh
</code></pre></div>
<p>If there is a problem, you will see that listed in the <code>REJECT REASONS</code> column. You might need to follow up on that by
using fdisk to remove partitions or using <code>ceph-volume lvm zap</code> to clean up devices.
I had no problems adding my three NVMe SSDs on the three Raspberry Pis.</p>
<p>If everything looks fine, you can just add all storage devices and create the ODS in one go:</p>
<div class="highlight"><pre><span></span><code>sudo<span class="w"> </span>cephadm<span class="w"> </span>shell<span class="w"> </span>ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>osd<span class="w"> </span>--all-available-devices
</code></pre></div>
<h2>Checking Result</h2>
<p>Everything should be up and running by now. You can check the results in the web dashboard which runs on the admin
node: <a href="https://ceph1:8443/">https://ceph1:8443/</a></p>
<p><img alt="Ceph Dashboard" src="http://127.0.0.1:8000/images/2024-12-26_ceph_dashboard.png"/></p>
<h2>Self-Criticism</h2>
<p>Working on the project, I noticed already some things I need to improve. Having the operating system running on the
SD cards was probably not the best idea. Especially the monitors will cause some serious wear and tear there. So I
probably need to change that at some point in the future. So I already eyeballed other PCIe to M.2 adapters which
support more than one SSD like <a href="https://www.waveshare.com/pcie-to-2-ch-m.2-hat-plus-b.htm">this one</a>. That way I could
have one SSD for the operating system and one for storage.
I admit that the managed 150W POE switch is quite overkill for that project. But I choose that one because I also
want to use it for other lab projects in the future. There are many cheaper options out there. For instance, Waveshare
is offering this <a href="https://www.waveshare.com/gigabit-poe-switch-120w.htm">cheap 120W POE switch</a> which would fully
suffice for that project.
Also, that DeskPi Rackmount is rather pricy for what it offers. So if you find cheaper options, I would rather go with
this.</p>
<h2>Outlook</h2>
<p>I will do a follow-up post on that topic for configuring the <a href="https://github.com/ceph/ceph-csi">Ceph CSI</a> for the
Kubernetes cluster as this proved not to be that straight forward.
I already threw in some hard drives in my Proxmox machine and plan to set up another Ceph cluster there. I am interested
to do some performance comparison between these two Ceph clusters.</p>
<h2>Related Articles</h2>
<ul>
<li><a href="http://127.0.0.1:8000/configuring-and-using-ceph-csi-driver-for-kubernetes.html">Configuring and using Ceph CSI Driver for Kubernetes</a></li>
<li><a href="http://127.0.0.1:8000/overhauling-my-ceph-cluster.html">Overhauling my Ceph cluster </a></li>
</ul>
</div>
</div>
</body>
</html>