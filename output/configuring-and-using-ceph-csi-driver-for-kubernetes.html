<!DOCTYPE html>

<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
<link href="http://gmpg.org/xfn/11" rel="profile"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<!-- Metadata -->
<meta content="A guide for configuring a Ceph cluster and Ceph CSI driver as Kubernetes storage solution" name="description"/>
<meta content="A guide for configuring a Ceph cluster and Ceph CSI driver as Kubernetes storage solution" property="og:description"/>
<meta content="Configuring and using Ceph CSI Driver for Kubernetes" property="og:title"/>
<meta content="article" property="og:type"/>
<meta content="http://127.0.0.1:8000/configuring-and-using-ceph-csi-driver-for-kubernetes.html" property="og:url"/>
<meta content="https://max-pfeiffer.github.io/blog/images/2025-03-16_ceph_csi_driver.png" property="og:image"/>
<!-- Enable responsiveness on mobile devices-->
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1" name="viewport">
<title>The Nerdy Tech Blog</title>
<!-- CSS -->
<link href="//fonts.googleapis.com/" rel="dns-prefetch"/>
<link href="//fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic|Abril+Fatface|PT+Sans:400,400italic,700&amp;subset=latin,latin-ext" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/poole.css" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/hyde.css" rel="stylesheet"/>
<link href="http://127.0.0.1:8000/theme/css/syntax.css" rel="stylesheet"/>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.1.7/css/fork-awesome.min.css" rel="stylesheet">
<!-- Feeds -->
<!-- Analytics -->
</link></meta><meta content="F3nYemi_dYYQckoOJ33wBp5BG4frj27_I6Iiianw594" name="google-site-verification"/><link href="/favicon.ico" rel="icon"/><link href="http://127.0.0.1:8000/configuring-and-using-ceph-csi-driver-for-kubernetes.html" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "The Nerdy Tech Blog", "item": "http://127.0.0.1:8000"}, {"@type": "ListItem", "position": 2, "name": "Configuring and using ceph csi driver for kubernetes", "item": "http://127.0.0.1:8000/configuring-and-using-ceph-csi-driver-for-kubernetes.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "Max Pfeiffer"}, "publisher": {"@type": "Organization", "name": "The Nerdy Tech Blog", "logo": {"@type": "ImageObject", "url": "https://max-pfeiffer.github.io/blog/images/avatar.jpeg"}}, "headline": "Configuring and using Ceph CSI Driver for Kubernetes", "about": "misc", "datePublished": "2025-03-16 11:00", "image": "https://max-pfeiffer.github.io/blog/images/2025-03-16_ceph_csi_driver.png"}</script></head>
<body class="theme-base-0d">
<div class="sidebar">
<div class="container sidebar-sticky">
<div class="sidebar-about">
<h1>
<a href="http://127.0.0.1:8000/">
<img class="profile-picture" src="http://127.0.0.1:8000/images/avatar.jpeg"/>
					The Nerdy Tech Blog
				</a>
</h1>
<p class="lead"></p>
<p class="lead"> </p>
<p></p>
</div>
<ul class="sidebar-nav">
<li><a href="http://127.0.0.1:8000/pages/about.html">About</a></li>
<li><a href="http://127.0.0.1:8000/pages/imprint.html">Imprint</a></li>
<li><a href="http://127.0.0.1:8000/pages/privacy.html">Privacy</a></li>
</ul>
<nav class="sidebar-social">
<a class="sidebar-social-item" href="https://github.com/max-pfeiffer" target="_blank">
<i class="fa fa-github"></i>
</a>
<a class="sidebar-social-item" href="http://127.0.0.1:8000/">
<i class="fa fa-rss"></i>
</a>
</nav>
<p class="sidebar-footer">Â© 2025 Max Pfeiffer</p>
</div>
</div> <div class="content container">
<div class="post">
<h1 class="post-title">Configuring and using Ceph CSI Driver for Kubernetes</h1>
<span class="post-date">Sun 16 March 2025</span>
<p>For my Kubernetes cluster, <a href="http://127.0.0.1:8000/ceph-cluster-with-raspberry-pi-5-and-nvme-ssds.html">I build a Ceph cluster with three Raspberry PI 5 as storage solution</a>. 
For hooking up that Ceph cluster to Kubernetes, you need to leverage the Kubernetes Container Storage Interface (CSI).
Kubernetes provides <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">an internal provisioner for Ceph Rados Block Devices (RBD)</a>.
But this provisioner is deprecated since Kubernetes v1.28. The Kubernetes team did not want to maintain it any more,
so the Ceph guys provide <a href="https://github.com/ceph/ceph-csi">Ceph CSI drivers</a> for interfacing with Kubernetes nowadays. In this
article, I will explain how I configured my Ceph cluster and how I installed and configured the
<a href="https://github.com/ceph/ceph-csi">Ceph CSI drivers</a> in my Kubernetes cluster.</p>
<p><img alt="2025-03-16_ceph_csi_driver.png" src="http://127.0.0.1:8000/images/2025-03-16_ceph_csi_driver.png"/></p>
<p>Ceph provides drivers for three different file systems for Kubernetes:
<a href="https://docs.ceph.com/en/reef/rbd/">RBD</a>,
<a href="https://docs.ceph.com/en/reef/cephfs/">CephFS</a> and
<a href="https://en.wikipedia.org/wiki/Network_File_System">NFS</a>.
The support for NFS is still in alpha state, so I will just focus on RBD and CephFS driver.
These two drivers support different access modes <strong>(file mode)</strong> for Kubernetes Volumes:</p>
<table>
<thead>
<tr>
<th>Volume Access Mode</th>
<th>Ceph CSI RBD</th>
<th>Ceph CSI CephFS</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReadWriteOnce (RWO)</td>
<td>supported</td>
<td>supported</td>
</tr>
<tr>
<td>ReadOnlyMany (ROX)</td>
<td>not supported</td>
<td>alpha state</td>
</tr>
<tr>
<td>ReadWriteMany (RWX)</td>
<td>not supported</td>
<td>supported</td>
</tr>
<tr>
<td>ReadWriteOncePod (RWOP)</td>
<td>alpha state</td>
<td>alpha state</td>
</tr>
</tbody>
</table>
<p>Besides the <strong>file mode</strong> support in the table above, please be aware that only that Ceph RBD driver will give you
<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support"><strong>raw block</strong> volume support for Kubernetes</a>.
Just in case you want to create some Persistent Volumes with raw block devices.</p>
<p>So if you want to create volumes with access mode <code>ReadWriteMany</code>, you need to install also the CephFS CSI driver in
your cluster. There is also a good video on YouTube explaining Ceph CSI drivers more in depth:</p>
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="315" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/kFE5C7o78Dk?si=VWGOF5tExixZ5laz" title="YouTube video player" width="560"></iframe>
<h2>Ceph cluster configuration</h2>
<p>In this guide I will assume that you have access to a shell with the <code>ceph</code> command. If you installed your cluster
with <code>cephadm</code> you can quickly spawn a shell like this:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>chephadm<span class="w"> </span>shell
</code></pre></div>
<h3>Configuration for RBD CSI Driver</h3>
<p>The configuration for the RBD CSI driver <a href="https://docs.ceph.com/en/reef/rbd/rbd-kubernetes/">is well documented</a>.
You need to create a new pool and initialize it:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>create<span class="w"> </span>kubernetes
pool<span class="w"> </span><span class="s1">'kubernetes'</span><span class="w"> </span>created
$<span class="w"> </span>rbd<span class="w"> </span>pool<span class="w"> </span>init<span class="w"> </span>kubernetes
</code></pre></div>
<p>Then create the user for the RBD CSI driver <a href="https://github.com/ceph/ceph-csi/blob/devel/docs/capabilities.md#rbd">with these capabilities</a>:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>auth<span class="w"> </span>get-or-create<span class="w"> </span>client.kubernetes<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>mon<span class="w"> </span><span class="s1">'profile rbd'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>osd<span class="w"> </span><span class="s1">'profile rbd pool=kubernetes'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>mgr<span class="w"> </span><span class="s1">'profile rbd pool=kubernetes'</span>
<span class="o">[</span>client.kubernetes<span class="o">]</span>
<span class="w">    </span><span class="nv">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">KAHDKLJDLiowejfnKjdflgmjdlfmreogkfrgmi9tmn</span><span class="o">==</span>
</code></pre></div>
<p>Grab that key. You will need it later for configuring the driver.</p>
<h3>Configuration for CephFS CSI Driver</h3>
<p><a href="https://docs.ceph.com/en/reef/cephfs/#getting-started-with-cephfs">Create a new CephFS file system by setting up a new volume</a>:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>fs<span class="w"> </span>volume<span class="w"> </span>create<span class="w"> </span>cephfs
</code></pre></div>
<p>The <a href="https://docs.ceph.com/en/reef/mgr/orchestrator/">Ceph Orchestrator</a> will do the following:</p>
<ul>
<li>create two new pools:</li>
<li>cephfs.cephfs.data</li>
<li>cephfs.cephfs.meta</li>
<li>create a new CephFS filesystem</li>
<li>create and run Metadata Servers (MDS)</li>
</ul>
<p>We also need to create <a href="https://docs.ceph.com/en/reef/cephfs/fs-volumes/#fs-subvolume-groups">a subvolume group</a>.
In this subvolume group, the CephFS CSI provisioner puts the Volumes we create dynamically later.</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>fs<span class="w"> </span>subvolumegroup<span class="w"> </span>create<span class="w"> </span>cephfs<span class="w"> </span>csi
</code></pre></div>
<p>Create a user for the CephFS CSI driver with <a href="https://github.com/ceph/ceph-csi/blob/devel/docs/capabilities.md#cephfs">these capabilities</a>:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>auth<span class="w"> </span>get-or-create<span class="w"> </span>client.kubernetes-cephfs<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>mgr<span class="w"> </span><span class="s1">'allow rw'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>osd<span class="w"> </span><span class="s1">'allow rw tag cephfs metadata=cephfs, allow rw tag cephfs data=cephfs'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>mds<span class="w"> </span><span class="s1">'allow r fsname=cephfs path=/volumes, allow rws fsname=cephfs path=/volumes/csi'</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>mon<span class="w"> </span><span class="s1">'allow r fsname=cephfs'</span>
<span class="o">[</span>client.kubernetes-cephfs<span class="o">]</span>
<span class="w">    </span><span class="nv">key</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">LKJUDwsdFHIeeUDHFreEUIDBNslkiedeklunUEDUEDJH</span><span class="o">==</span>
</code></pre></div>
<p>Also grab that key.</p>
<h2>Installing Ceph CSI drivers</h2>
<p>Ceph guys provide Helm charts for the <a href="https://github.com/ceph/ceph-csi/tree/devel/charts/ceph-csi-rbd">RBD CSI driver</a>
and the <a href="https://github.com/ceph/ceph-csi/tree/devel/charts/ceph-csi-cephfs">CephFS CSI driver</a>. We will use these
Helm charts to install the drivers in our cluster in <code>csi</code> namespace.</p>
<p>When you are running a Kubernetes version, that enforces <a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">pod security admission</a>,
make sure to add the <code>pod-security.kubernetes.io/enforce = privileged</code> label to that namespace. Otherwise, the
driver's pods will not come up.</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>csi
$<span class="w"> </span>kubectl<span class="w"> </span>label<span class="w"> </span>namespace<span class="w"> </span>csi<span class="w"> </span>pod-security.kubernetes.io/enforce<span class="o">=</span>privileged
</code></pre></div>
<p>Pull some more configuration information from your cluster:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>ceph<span class="w"> </span>mon<span class="w"> </span>dump
epoch<span class="w"> </span><span class="m">3</span>
fsid<span class="w"> </span>4637534klj-4j44-456d-344d-348465ituhfnf
last_changed<span class="w"> </span><span class="m">2025</span>-03-08T18:21:49.254139+0000
created<span class="w"> </span><span class="m">2025</span>-03-08T18:09:39.262040+0000
min_mon_release<span class="w"> </span><span class="m">19</span><span class="w"> </span><span class="o">(</span>squid<span class="o">)</span>
election_strategy:<span class="w"> </span><span class="m">1</span>
<span class="m">0</span>:<span class="w"> </span><span class="o">[</span>v2:192.168.30.10:3300/0,v1:192.168.30.10:6789/0<span class="o">]</span><span class="w"> </span>mon.ceph1
<span class="m">1</span>:<span class="w"> </span><span class="o">[</span>v2:192.168.30.11:3300/0,v1:192.168.30.11:6789/0<span class="o">]</span><span class="w"> </span>mon.ceph2
<span class="m">2</span>:<span class="w"> </span><span class="o">[</span>v2:192.168.30.12:3300/0,v1:192.168.30.12:6789/0<span class="o">]</span><span class="w"> </span>mon.ceph3
dumped<span class="w"> </span>monmap<span class="w"> </span>epoch<span class="w"> </span><span class="m">3</span>
</code></pre></div>
<h3>RBD CSI driver</h3>
<p>We need to prepare our <code>values.yaml</code> file for the RBD driver installation. We insert the configuration data which
we produced in the above steps:</p>
<div class="highlight"><pre><span></span><code><span class="nt">csiConfig</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">clusterID</span><span class="p">:</span><span class="w"> </span><span class="s">"4637534klj-4j44-456d-344d-348465ituhfnf"</span>
<span class="w">     </span><span class="nt">monitors</span><span class="p">:</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.10:6789"</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.11:6789"</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.12:6789"</span>

<span class="nt">storageClass</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Specifies whether the storageclass should be created</span>
<span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="c1"># (required) String representing a Ceph cluster to provision storage from.</span>
<span class="w">  </span><span class="c1"># Should be unique across all Ceph clusters in use for provisioning,</span>
<span class="w">  </span><span class="c1"># cannot be greater than 36 bytes in length, and should remain immutable for</span>
<span class="w">  </span><span class="c1"># the lifetime of the StorageClass in use.</span>
<span class="w">  </span><span class="nt">clusterID</span><span class="p">:</span><span class="w"> </span><span class="s">"4637534klj-4j44-456d-344d-348465ituhfnf"</span>
<span class="w">  </span><span class="c1"># (required) Ceph pool into which the RBD image shall be created</span>
<span class="w">  </span><span class="c1"># (optional) if topologyConstrainedPools is provided</span>
<span class="w">  </span><span class="c1"># eg: pool: replicapool</span>
<span class="w">  </span><span class="nt">pool</span><span class="p">:</span><span class="w"> </span><span class="s">"kubernetes"</span>
<span class="w">  </span><span class="nt">annotations</span><span class="p">:</span>
<span class="w">    </span><span class="nt">storageclass.kubernetes.io/is-default-class</span><span class="p">:</span><span class="w"> </span><span class="s">"true"</span>

<span class="c1"># Mount the host /etc/selinux inside pods to support</span>
<span class="c1"># selinux-enabled filesystems</span>
<span class="nt">selinuxMount</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="nt">secret</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Specifies whether the secret should be created</span>
<span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">userID</span><span class="p">:</span><span class="w"> </span><span class="s">"kubernetes"</span>
<span class="w">  </span><span class="nt">userKey</span><span class="p">:</span><span class="w"> </span><span class="s">"KAHDKLJDLiowejfnKjdflgmjdlfmreogkfrgmi9tmn=="</span>

<span class="c1"># Name of the configmap used for ceph.conf</span>
<span class="nt">cephConfConfigMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-rbd-config</span>
<span class="c1"># Name of the configmap used for state</span>
<span class="nt">configMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-csi-rbd-config</span>
<span class="c1"># Name of the configmap used for encryption kms configuration</span>
<span class="nt">kmsConfigMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-csi-rbd-encryption-kms-config</span>
</code></pre></div>
<p>Please not two peculiarities here:</p>
<ol>
<li>With the annotation <code>storageclass.kubernetes.io/is-default-class: "true"</code> I made the RBD driver the default storage class</li>
<li>Using <a href="https://www.talos.dev/">Talos Linux</a> as Kubernetes platform, I had to set <code>selinuxMount: false</code> as this was not supported</li>
<li>As we install both CSI drivers in the same namespace, we need to customize the names of the ConfigMaps</li>
</ol>
<p>We can install the RBD CSI driver now:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>ceph-csi<span class="w"> </span>https://ceph.github.io/csi-charts
$<span class="w"> </span>helm<span class="w"> </span>install<span class="w"> </span>ceph-csi-rbd<span class="w"> </span>ceph-csi/ceph-csi-rbd<span class="w"> </span>-f<span class="w"> </span>values.yaml<span class="w"> </span>--namespace<span class="w"> </span>csi<span class="w"> </span>
</code></pre></div>
<p>Ceph RRB CSI driver pods should be up and running after a few seconds. Then check for their status.</p>
<h3>CephFS CSI driver</h3>
<p>Also for this driver we need to prepare the <code>values.yaml</code> file:</p>
<div class="highlight"><pre><span></span><code><span class="nt">csiConfig</span><span class="p">:</span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">clusterID</span><span class="p">:</span><span class="w"> </span><span class="s">"4637534klj-4j44-456d-344d-348465ituhfnf"</span>
<span class="w">     </span><span class="nt">monitors</span><span class="p">:</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.10:6789"</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.11:6789"</span>
<span class="w">       </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">"192.168.30.12:6789"</span>
<span class="w">     </span><span class="nt">cephFS</span><span class="p">:</span>
<span class="w">       </span><span class="nt">subvolumeGroup</span><span class="p">:</span><span class="w"> </span><span class="s">"csi"</span>

<span class="nt">storageClass</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Specifies whether the storageclass should be created</span>
<span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="c1"># (required) String representing a Ceph cluster to provision storage from.</span>
<span class="w">  </span><span class="c1"># Should be unique across all Ceph clusters in use for provisioning,</span>
<span class="w">  </span><span class="c1"># cannot be greater than 36 bytes in length, and should remain immutable for</span>
<span class="w">  </span><span class="c1"># the lifetime of the StorageClass in use.</span>
<span class="w">  </span><span class="nt">clusterID</span><span class="p">:</span><span class="w"> </span><span class="s">"4637534klj-4j44-456d-344d-348465ituhfnf"</span>
<span class="w">  </span><span class="c1"># (required) CephFS filesystem name into which the volume shall be created</span>
<span class="w">  </span><span class="c1"># eg: fsName: myfs</span>
<span class="w">  </span><span class="nt">fsName</span><span class="p">:</span><span class="w"> </span><span class="s">"cephfs"</span>

<span class="c1"># Mount the host /etc/selinux inside pods to support</span>
<span class="c1"># selinux-enabled filesystems</span>
<span class="nt">selinuxMount</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="nt">secret</span><span class="p">:</span>
<span class="w">  </span><span class="c1"># Specifies whether the secret should be created</span>
<span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">  </span><span class="nt">adminID</span><span class="p">:</span><span class="w"> </span><span class="s">"kubernetes-cephfs"</span>
<span class="w">  </span><span class="nt">adminKey</span><span class="p">:</span><span class="w"> </span><span class="s">"LKJUDwsdFHIeeUDHFreEUIDBNslkiedeklunUEDUEDJH=="</span>

<span class="c1"># Name of the configmap used for ceph.conf</span>
<span class="nt">cephConfConfigMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-cephfs-config</span>
<span class="c1"># Name of the configmap used for state</span>
<span class="nt">configMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-csi-cephfs-config</span>
<span class="c1"># Name of the configmap used for encryption kms configuration</span>
<span class="nt">kmsConfigMapName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ceph-csi-cephfs-encryption-kms-config</span>
</code></pre></div>
<p>Please note that we need to specify <code>adminID</code> and <code>adminKey</code> for dynamic provisioning of PersistentVolumes with this
CSI driver.</p>
<p>Install the CephFS driver with Helm:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>ceph-csi<span class="w"> </span>https://ceph.github.io/csi-charts
$<span class="w"> </span>helm<span class="w"> </span>install<span class="w"> </span>ceph-csi-cephfs<span class="w"> </span>ceph-csi/ceph-csi-cephfs<span class="w"> </span>-f<span class="w"> </span>values.yaml<span class="w"> </span>--namespace<span class="w"> </span>csi<span class="w"> </span>
</code></pre></div>
<h2>Check the results</h2>
<p>You should have both CSI drivers up and running by now. Let's check on the installation:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>kubectl<span class="w"> </span>-n<span class="w"> </span>csi<span class="w"> </span>get<span class="w"> </span>pods
NAME<span class="w">                                           </span>READY<span class="w">   </span>STATUS<span class="w">    </span>RESTARTS<span class="w">        </span>AGE
ceph-csi-cephfs-nodeplugin-kkkdq<span class="w">               </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-cephfs-nodeplugin-lzn79<span class="w">               </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-cephfs-nodeplugin-w4qpl<span class="w">               </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-cephfs-provisioner-58744fdf76-59dnk<span class="w">   </span><span class="m">5</span>/5<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-cephfs-provisioner-58744fdf76-66rzk<span class="w">   </span><span class="m">5</span>/5<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-cephfs-provisioner-58744fdf76-tslzf<span class="w">   </span><span class="m">5</span>/5<span class="w">     </span>Running<span class="w">   </span><span class="m">3</span><span class="w"> </span><span class="o">(</span>3h55m<span class="w"> </span>ago<span class="o">)</span><span class="w">   </span>3h57m
ceph-csi-rbd-nodeplugin-8vz7g<span class="w">                  </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-rbd-nodeplugin-bhpqh<span class="w">                  </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-rbd-nodeplugin-nxkfz<span class="w">                  </span><span class="m">3</span>/3<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-rbd-provisioner-bd78f48fd-2g9nf<span class="w">       </span><span class="m">7</span>/7<span class="w">     </span>Running<span class="w">   </span><span class="m">3</span><span class="w"> </span><span class="o">(</span>3h55m<span class="w"> </span>ago<span class="o">)</span><span class="w">   </span>3h57m
ceph-csi-rbd-provisioner-bd78f48fd-ht8tc<span class="w">       </span><span class="m">7</span>/7<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m
ceph-csi-rbd-provisioner-bd78f48fd-l2kc2<span class="w">       </span><span class="m">7</span>/7<span class="w">     </span>Running<span class="w">   </span><span class="m">0</span><span class="w">               </span>3h57m

$<span class="w"> </span>kubectl<span class="w"> </span>get<span class="w"> </span>storageclass<span class="w">                     </span>
NAME<span class="w">                   </span>PROVISIONER<span class="w">           </span>RECLAIMPOLICY<span class="w">   </span>VOLUMEBINDINGMODE<span class="w">   </span>ALLOWVOLUMEEXPANSION<span class="w">   </span>AGE
csi-cephfs-sc<span class="w">          </span>cephfs.csi.ceph.com<span class="w">   </span>Delete<span class="w">          </span>Immediate<span class="w">           </span><span class="nb">true</span><span class="w">                   </span>3h57m
csi-rbd-sc<span class="w"> </span><span class="o">(</span>default<span class="o">)</span><span class="w">   </span>rbd.csi.ceph.com<span class="w">      </span>Delete<span class="w">          </span>Immediate<span class="w">           </span><span class="nb">true</span><span class="w">                   </span>3h57m
</code></pre></div>
<p>Pods of the CSI drivers should now be running in <code>csi</code> namespace, and you should have two StorageClasses available.
Let's create a PersistentVolumeClaim with each of the StorageClasses:</p>
<div class="highlight"><pre><span></span><code>$<span class="w"> </span>cat<span class="w"> </span><span class="s">&lt;&lt;EOF &gt; test-rbd-pvc.yaml</span>
<span class="s">---</span>
<span class="s">apiVersion: v1</span>
<span class="s">kind: PersistentVolumeClaim</span>
<span class="s">metadata:</span>
<span class="s">  name: test-rbd-pvc</span>
<span class="s">spec:</span>
<span class="s">  accessModes:</span>
<span class="s">    - ReadWriteOnce</span>
<span class="s">  volumeMode: Filesystem</span>
<span class="s">  resources:</span>
<span class="s">    requests:</span>
<span class="s">      storage: 1Gi</span>
<span class="s">  storageClassName: csi-rbd-sc</span>
<span class="s">EOF</span>
$<span class="w"> </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>test-rbd-pvc.yaml
$<span class="w"> </span>kubectl<span class="w"> </span>get<span class="w"> </span>pvc<span class="w"> </span>test-rbd-pvc<span class="w">      </span>
NAME<span class="w">           </span>STATUS<span class="w">   </span>VOLUME<span class="w">                                     </span>CAPACITY<span class="w">   </span>ACCESS<span class="w"> </span>MODES<span class="w">   </span>STORAGECLASS<span class="w">   </span>VOLUMEATTRIBUTESCLASS<span class="w">   </span>AGE
test-rbd-pvc<span class="w">   </span>Bound<span class="w">    </span>pvc-dbb3d97e-eab5-45b9-81e9-3140eadc8a49<span class="w">   </span>1Gi<span class="w">        </span>RWO<span class="w">            </span>csi-rbd-sc<span class="w">     </span>&lt;unset&gt;<span class="w">                 </span>20s
</code></pre></div>
<p>If the PVC became bound, we are good.</p>
<p>Let's check if it also works with the CephFS StorageClass:</p>
<div class="highlight"><pre><span></span><code><span class="sb">```</span>shell
$<span class="w"> </span>cat<span class="w"> </span><span class="s">&lt;&lt;EOF &gt; test-cephfs-pvc.yaml</span>
<span class="s">---</span>
<span class="s">apiVersion: v1</span>
<span class="s">kind: PersistentVolumeClaim</span>
<span class="s">metadata:</span>
<span class="s">  name: test-cephfs-pvc</span>
<span class="s">spec:</span>
<span class="s">  accessModes:</span>
<span class="s">    - ReadWriteMany</span>
<span class="s">  volumeMode: Filesystem</span>
<span class="s">  resources:</span>
<span class="s">    requests:</span>
<span class="s">      storage: 1Gi</span>
<span class="s">  storageClassName: csi-cephfs-sc</span>
<span class="s">EOF</span>
$<span class="w"> </span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>test-cephfs-pvc.yaml
$<span class="w"> </span>kubectl<span class="w"> </span>get<span class="w"> </span>pvc<span class="w"> </span>test-cephfs-pvc<span class="w">      </span>
NAME<span class="w">              </span>STATUS<span class="w">   </span>VOLUME<span class="w">                                     </span>CAPACITY<span class="w">   </span>ACCESS<span class="w"> </span>MODES<span class="w">   </span>STORAGECLASS<span class="w">    </span>VOLUMEATTRIBUTESCLASS<span class="w">   </span>AGE
test-cephfs-pvc<span class="w">   </span>Bound<span class="w">    </span>pvc-cd5c3d81-9596-4a44-9f11-250fa30ad152<span class="w">   </span>1Gi<span class="w">        </span>RWO<span class="w">            </span>csi-cephfs-sc<span class="w">   </span>&lt;unset&gt;<span class="w">                 </span>3s
</code></pre></div>
<p>When this PVC also became bound, we are happy campers. Now you can use these two new StorageClasses for applications
you run in your Kubernetes cluster.</p>
<h2>Related Articles</h2>
<ul>
<li><a href="http://127.0.0.1:8000/ceph-cluster-with-raspberry-pi-5-and-nvme-ssds.html">Ceph Cluster with Raspberry Pi 5 and NVMe SSDs</a> </li>
<li><a href="http://127.0.0.1:8000/overhauling-my-ceph-cluster.html">Overhauling my Ceph cluster </a></li>
</ul>
</div>
</div>
</body>
</html>